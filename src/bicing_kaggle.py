# -*- coding: utf-8 -*-
"""Bicing_Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nwn6WYBHCMxbauU_KhVnLhY4ux4hAr8t

# Project Capstone                
## Model and train

La **les dades del bicing** ja han estat descarregades de la web de l'ajuntament i preprocessades.
Ara tenim el dataframe preparat per dividir -estratificant prèviament o no, entre train i test(validation)

Haurem d'anar afegint les variables addicionals que volguem i estandaritzar i/o d'altres transformacions..
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
from joblib import dump, load
import plotly.express as px

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd

from sklearn import set_config
# to obtain a pandas df to the output of 'fit_transform' instead a numpy arrary
#set_config(transform_output="pandas")

import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
# %matplotlib inline

# import pel training

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error,r2_score

from sklearn.ensemble import RandomForestRegressor
#from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import neighbors
from sklearn.linear_model import LinearRegression

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.impute import SimpleImputer #per assignar la mitjana ó ... on hi ha NANS

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

#from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

from datetime import datetime

seed=42
np.random.seed(seed)

# import custom library

#from DataClean import clean_data_pipeline
#from DataReestructure import transform_data_pipeline
from DataLoad import restore_stations_loc_info, load_holidaysBCN
from MeteoBCN import load_meteocat_data, AssignWeatherVariables

"""### Anem a entrenar ###"""

#recuperem el fitxer de training
dftotrain=pd.read_csv('/content/drive/MyDrive/Bicing/Bicing_ToTrain.csv')

dftotrain.head()

dftotrain.isnull().values.any()

#dftotrain.shape  #shape abans del problema del 24Gener2020: (2466422, 12)

dftotrain[['percentage_docks_available']].isnull().values.any()

sns.histplot(dftotrain['percentage_docks_available'],bins=10)
plt.show()

def compute_score(y_true, y_pred):
    return {
        "R2": f"{r2_score(y_true, y_pred):.3f}",
        "MSE": f"{mean_squared_error(y_true, y_pred):.3f}",
    }

"""#### Preparant el trainig amb estratificació ####

"""

dftotrain["disponibilitat"] = pd.cut(dftotrain["percentage_docks_available"],
                               bins=[-0.01, 0.0, 0.2, 0.6, 0.8, 1.],
                               labels=['ple','força ple', 'mig buit', 'quasi buit', 'buit'])

dftotrain.loc[dftotrain["disponibilitat"]=='força ple']

dftotrain['disponibilitat'].unique()

"""'''

#probem a estratificar amb varies variables:

#group=['station_id','month','hour','day'] no funciona:ValueError: The least populated class in y has only
1 member, which is too few. The minimum number of groups for any class cannot be less than 2.

#group=['year','month','hour','day'] : MSE dona semblant,però un xic més alt

group=['month','hour','day'] : millor que sense res, pero inferior que els altres

X_train, X_test, y_train, y_test = train_test_split(dftotrain,y,test_size=.2,random_state=seed,shuffle=True,stratify=dftotrain[group])
X_train = X_train[features]
X_test = X_test[features]

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
#Make predictions
y_train_pred_lr = lin_reg.predict(X_train)
y_test_pred_lr = lin_reg.predict(X_test)
#Compute MSE for training and testing sets
print('MSE (train | test):')
print(mean_squared_error(y_train_pred_lr, y_train), mean_squared_error(y_test_pred_lr, y_test))
'''"

### Afegim features al training ###
"""

dftotrain.head(3)

dftotrain.shape

dftotrain.describe()

class InfoAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self,infoDF=pd.DataFrame()):
        self.infoDF = pd.read_csv('/content/Informacio_Estacions_BicingNou.csv', encoding='latin-1', delimiter=";")

    def fit(self, X, y=None):
        return self  # nothing else to do

    def transform(self, X:pd.DataFrame, predict=False):
        if X.shape[1] == 8: #estem en el cas del submission file
            X = X.merge(self.infoDF[['station_id','capacity','altitude','post_code','n_transp_500m','min_dist_to_beach']],on=['station_id'],how='left')
            X.insert(1, "year", 2023)
        else:
            X = X.merge(self.infoDF[['station_id','post_code','n_transp_500m','min_dist_to_beach']],on=['station_id'],how='left')
        return X

bicing_platges=[400,124,32,31,33,446,424,398,11,12,125,397,170,171,190,173,116,39,40]
bicing_uni=[422,433,430,435,432,429,302]
bicing_alta = [212,213,214,215,216,217,219,227,228,229,230,231,276,279,282,283,284,288,290,291,292,293,294,295,297,\
299,301,302,303,304,305,306,307,308,318,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,\
338,356,357,429,431,454,460,462,463,466,467,468,469,470,471,472,473,474,476,477,478,479,480,481,482,483,484,485,\
486,487,488,489,490,491,493,497,498,499,500,501,502,503,504,505,506,507,508,512,513,518,519]
bicing_pcatalunya=[394,79,78,406,287,66,65,64,62,63,395,494,15,412,105]

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self,festes=[]):
        self.festes = load_holidaysBCN()
    def fit(self, X, y=None):
        return self  # nothing else to do

    def transform(self, X:pd.DataFrame):
        X['weekend']= pd.to_datetime(X[['year','month','day']]).dt.weekday > 4
        X['peekhour'] = X['hour'].apply(lambda x: 1 if ((x in range(8,10)) | (x in range(17,19)) ) else 0)
        # i ara fem producte dfclean22['peekhour'] * dfclean22['workday'] pq només aplica si és laborable
        X['peekhour']= X.peekhour*(~X.weekend)
        # ara que ja hem actualitzat peekhour, podem convertir weekend -booleà- a integer
        X.weekend = X.weekend.replace({True: 1, False: 0})
        X['holiday']= pd.to_datetime(X[['day','month','year']]).isin(self.festes)
        X.holiday = X.holiday.replace({True: 1, False: 0})
        X['season']= X.month.apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 10 else 4)
        X['zone'] =  X.station_id.apply(lambda x: 'platges' if x in bicing_platges else 'uni' if x in bicing_uni \
            else 'alta' if x in bicing_alta else 'pcatalunya' if x in bicing_pcatalunya else 'other')
        return X

class WheatherAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self,metDF=pd.DataFrame()):
        self.metDF = pd.read_csv('/content/All_MeteoBCN.csv')
    def fit(self, X, y=None):
        return self  # nothing else to do

    def transform(self, X:pd.DataFrame):
        X = X.merge(self.metDF, on=['year','month','day','hour'],how='left')
        return X

attrib_outliers=['n_transp_500m','HR','VV10','P','RS']

class Replace_Outliers_2_NaN(BaseEstimator, TransformerMixin):
    def __init__(self): # no *args or **kargs
        self.q75=[]
        self.q25=[]
        return
    def fit(self, X, y=None):
        for i in range(X.shape[1]):
            q7,q3 = np.nanpercentile(X.iloc[:,i],[70,30])
            (self.q75).append(q7)
            (self.q25).append(q3)
        return self
    def transform(self, X):
        for i in range(X.shape[1]):
            iqr=self.q75[i]-self.q25[i] #Inter quantile range
            min=self.q25[i]-(iqr*1.5) #inner fence
            max=self.q75[i]+(iqr*1.5) #outer fence
            X.iloc[(X.iloc[:,i] < min)] = np.nan  #Replace with NA
            X.iloc[(X.iloc[:,i] > max)] = np.nan  #Replace with NA
        return X

add_features = Pipeline([
    ('info',InfoAttributesAdder()),
    ('comb',CombinedAttributesAdder()),
    ('weather',WheatherAttributesAdder()),

])

train_attribs_added = add_features.fit_transform(dftotrain)

dump(add_features, '/content/bicing_add_features.joblib')

train_attribs_added.columns[train_attribs_added.isna().any()]

train_attribs_added.info()

#Veiem la correlació entre todas las features numèriques
plt.figure(figsize=(18,12))
#attribs_corr=train_attribs_added[]
corr = train_attribs_added.corr()
sns.heatmap(data=corr, annot=True, fmt = '.2g', linewidth=1)
plt.show()

bicing = train_attribs_added.copy()
bicing

bicing.hist(bins=30, figsize=(20,15))
plt.show()

#mirem les dades de precipitació en detall
bicing.loc[bicing['PPT']>0,'PPT'].hist(bins=[0,1,2,3,4,5],figsize=(20,15))
plt.show()

"""**Fem split amb estratificació**"""

labels=bicing['percentage_docks_available'].copy()

labels.isna().any()

X_train, X_test, y_train, y_test = train_test_split(bicing,labels,test_size=.2,random_state=seed,shuffle=True,stratify=bicing['disponibilitat'])

#eliminem la columna target i la categòrica que hem fet servir per estratificar
for set_ in (X_train, X_test):
    set_.drop(columns=['percentage_docks_available','disponibilitat'],inplace=True)

print('Training Shape: X {} , y: {}'.format(X_train.shape,y_train.shape))
print('Testing Shape: X {} , y: {}'.format(X_test.shape,y_test.shape))

X_train.columns[X_train.isna().any()]

X_train

X_train.info()

"""**Transformacions**"""

# variables numèriques

num1_attribs=['altitude','RS','n_transp_500m']
num1_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('log',FunctionTransformer(np.log1p)),
        ('std_scaler', StandardScaler()),
    ])

num2_attribs = ['capacity','month','hour','year','HR','T','min_dist_to_beach']
num2_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('std_scaler', StandardScaler()),
    ])

num3_attribs=['ctx-4','ctx-3','ctx-2','ctx-1','weekend','peekhour','holiday','season'] #passthrough
num4_attribs=['n_transp_500m','HR','RS']
num4_pipeline = Pipeline([
        ('replace_outliers',Replace_Outliers_2_NaN()),
        ('imputer', SimpleImputer(strategy="median")),
        ('std_scaler', StandardScaler()),
    ])

#variables categòriques : ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),
#per stationid haviem provat onehotencoder però genera 505 variables i quan executem RandomForest triga massa en
# executar-se

def transform_Num_to_Cat(data):
    data = data.astype('object')
    return data

cat1_attribs = ['station_id','post_code']
cat1_pipeline = Pipeline([
        ('transform_to_Cat',FunctionTransformer(func = transform_Num_to_Cat,validate=False)),
        ('imputer', SimpleImputer(strategy="constant",fill_value='Unknown')),
        ('ordinal_encoder', OrdinalEncoder()),
    ])

#('ordinal_encoder', OrdinalEncoder(handle_unknown='ignore')),
cat2_attribs=['zone'] #ja és categòrica
cat2_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="constant",fill_value='Unknown')),
    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),
    ])

full_pipeline = ColumnTransformer([
        ("num1", num1_pipeline, num1_attribs),
        ("num2", num2_pipeline, num2_attribs),
        ("cat1", cat1_pipeline, cat1_attribs),
        ("cat2", cat2_pipeline, cat2_attribs),
        ("num3", 'passthrough', num3_attribs),
        ("num4", num4_pipeline, num4_attribs),
        ])

num1_attribs+num2_attribs+cat1_attribs+cat2_attribs+num3_attribs

"""Fit and Transform train"""

train_prepared = full_pipeline.fit_transform(X_train[num1_attribs+num2_attribs+cat1_attribs+cat2_attribs+num3_attribs])

train_prepared

dump(full_pipeline, '/content/bicing_columntransformer_test.joblib')

train_prepared.shape

"""Transform test"""

test_prepared=full_pipeline.transform(X_test[num1_attribs+num2_attribs+cat1_attribs+cat2_attribs+num3_attribs])

test_prepared.shape

import xgboost as xgb
xgb_model = xgb.XGBRegressor(n_estimators=400)

xgb_model.fit(train_prepared, y_train)

 # Make predictions
y_train_xgb = xgb_model.predict(train_prepared)

#test_prepared=full_pipeline.fit_transform(X_test)

y_test_xgb = xgb_model.predict(test_prepared)

# Compute MSE for training and testing sets
print('MSE (train | test):')
print(mean_squared_error(y_train_xgb, y_train), mean_squared_error(y_test_xgb, y_test))
print('R2 (train | test):')
print(r2_score(y_train_xgb, y_train), r2_score(y_test_xgb, y_test))

"""ESTIMADOR DE 400:
MSE (train | test):
0.009157074881250598 0.009516862346692447
R2 (train | test):
0.8728221437768074 0.8678235449741648
"""

dump(xgb_model, '/content/bicing_xgb_model.joblib')

# param_grid = {
#     'n_estimators': [100, 200, 300],  # Number of boosting rounds
#     'learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage
#     'max_depth': [3, 4, 5],  # Maximum depth of each tree
#     'min_child_weight': [1, 2, 3]  # Minimum sum of instance weight needed in a child
# }

# train across 5 folds, that's a total of (12+6)*5=90 rounds of training
# grid_search = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5,
#                            scoring='neg_mean_squared_error',
#                            return_train_score=True)
# grid_search.fit(train_prepared, y_train)

"""### Predir el submission set ###"""

### Predir el submission set ###
add_features = Pipeline([
        ('info',InfoAttributesAdder()),
        ('comb', CombinedAttributesAdder()),
        ('weather', WheatherAttributesAdder())
    ])

submission_set = pd.read_csv('/content/drive/MyDrive/Bicing/metadata_sample_submission.csv',index_col=0)
submission_set

submission_set = add_features.fit_transform(submission_set)
submission_set

X_pred = full_pipeline.transform(submission_set)
X_pred

y_pred = xgb_model.predict(X_pred)
df_output = pd.DataFrame(y_pred)
df_output = df_output.reset_index()
df_output.columns = ['index','percentage_docks_available']
avui = datetime.today()
name = '/content/drive/MyDrive/Bicing/submission-{}-{}-{}-{}.csv'.format(avui.year,avui.month,avui.day,avui.hour)
df_output.to_csv(name,index=False)

## Best features according to Xgboost
feature_importances = xgb_model.feature_importances_

attributes = num1_attribs+num2_attribs+cat1_attribs+cat2_attribs+num3_attribs+num4_attribs
#cat_encoder = full_pipeline.named_transformers_["cat1"]
cat_one_hot_attribs = list(full_pipeline.named_transformers_["cat2"]['one_hot_encoder'].categories_[0])
#attributes = ["log_" + t for t in num1_attribs ] + num2_attribs + extra_attribs + cat_one_hot_attribs
attributes = num1_attribs+num2_attribs+cat1_attribs+cat_one_hot_attribs+num3_attribs+num4_attribs
list_sorted=sorted(zip(feature_importances, attributes), reverse=True)
list_sorted

#visualitzar-ho en un plot
features = pd.DataFrame({
    'Feature' : attributes,
    'Weight' : feature_importances
})
features=features.sort_values(by = 'Weight', ascending = True)
fig=px.bar(data_frame = features, x = 'Weight', y='Feature', color = 'Weight',
       title = 'Feature Importance')
fig.update_layout(xaxis={'categoryorder': 'array', 'categoryarray': attributes})
fig.update_layout(height=600, width=800)
fig.show()

# lin_reg = LinearRegression()
# lin_reg.fit(train_prepared, y_train)

# # Make predictions
# y_train_pred_lr = lin_reg.predict(train_prepared)
# y_test_pred_lr = lin_reg.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_pred_lr, y_train), mean_squared_error(y_test_pred_lr, y_test))

# print('R2 (train | test):')
# print(r2_score(y_train_pred_lr, y_train), r2_score(y_test_pred_lr, y_test))

"""i season i onehot zone

**MSE (train | test):** 0.011952616630703662 0.011753405949056097
**R2 (train | test):** 0.8286351767998206 0.8315535333730344

i holiday i cat post_code

**MSE (train | test):** 0.011954629042157616 0.011755419631984516
**R2 (train | test):** 0.8286042587518722 0.831524910024789

hora i mes i dist_platges

**MSE (train | test):** 0.011883371285297036 0.011686851128218663
**R2 (train | test):** 0.8297995323180921 0.8326980913954616
"""

# forest_reg = RandomForestRegressor(random_state=seed) #afegir n_estimators=20 que per defecte és 100
# forest_reg.fit(train_prepared, y_train)
#  # Make predictions
# y_train_fr = forest_reg.predict(train_prepared)

# #test_prepared=full_pipeline.fit_transform(X_test)

# y_test_fr = forest_reg.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_fr, y_train), mean_squared_error(y_test_fr, y_test))
# print('R2 (train | test):')
# print(r2_score(y_train_fr, y_train), r2_score(y_test_fr, y_test))

"""month,day,etc categòriques:
MSE (train | test):
0.0014684562628317042 0.01033163066795174

anterior:
MSE (train | test):
0.001468817643610355 0.010342074374592582

dos anterior:
MSE (train | test):
0.0014705172388749916 0.010343680402105112

tres anterior
MSE (train | test):
0.0015281045979033243 0.010738671476996902
"""

# grad_reg = GradientBoostingRegressor(n_estimators=100,random_state=seed,max_features=1.0, criterion='squared_error')
# grad_reg.fit(train_prepared, y_train)

# # Make predictions
# y_train_gr = grad_reg.predict(train_prepared)

# y_test_gr = grad_reg.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_gr, y_train), mean_squared_error(y_test_gr, y_test))
# print('R2 (train | test):')
# print(r2_score(y_train_gr, y_train), r2_score(y_test_gr, y_test))

# import xgboost as xgb
# xgb_reg = xgb.XGBRegressor()
# xgb_reg.fit(train_prepared, y_train)

# # Make predictions
# y_train_xg = xgb_reg.predict(train_prepared)
# y_test_xg = xgb_reg.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_xg, y_train), mean_squared_error(y_test_xg, y_test))
# print('R2 (train | test):')
# print(r2_score(y_train_xg, y_train), r2_score(y_test_xg, y_test))

# n_neighbors = 3
# knn_reg = neighbors.KNeighborsRegressor(n_neighbors)
# knn_reg.fit(train_prepared, y_train)

# # Make predictions
# y_train_kn = knn_reg.predict(train_prepared)

# y_test_kn = knn_reg.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_kn, y_train), mean_squared_error(y_test_kn, y_test))
# print('R2 (train | test):')
# print(r2_score(y_train_kn, y_train), r2_score(y_test_kn, y_test))

## lets compare all of them
# allscores=[]
# def display_scores(scores, model_name = None):
#     if(model_name):
#         print("----",model_name,"----")
#     allscores.append([model_name,scores.mean(),scores.std()])
#     print("Mean:", scores.mean())
#     print("Standard deviation:", scores.std())

# lin_reg2 = LinearRegression()
# knn_reg2 = neighbors.KNeighborsRegressor(3)
# forest_reg2 = RandomForestRegressor(random_state=seed)
# grad_reg2 = GradientBoostingRegressor(n_estimators=50,random_state=seed,max_features=1.0, criterion='squared_error')

# models = [(lin_reg2,"lin_reg"),
#           (knn_reg2,"KNN-Regressor"),
#           (forest_reg2,'Random Forest'),
#           (grad_reg2,'Gradient Boosting')]
# modelScores=pd.DataFrame(models,columns=['Model','Name','Mean','Std'])

# for model in models:
#     scores = cross_val_score(model[0], train_prepared, y_train, scoring="neg_mean_squared_error", cv=5)
#     display_scores(-scores,model[1])

# allscores= pd.DataFrame(allscores,columns=['Name','Mean','Std'])

# best_score=0
# for idx,model in models:
#     if model[0].score.mean(train_prepared, y_train) > best_score:
#         best_score=val[0].score.mean(train_prepared, y_train)
#         best_model_idx = idx
# print("Best model for bicing prediction: %s | MSE: %f\n",  models[best_model_idx][1],-best_score)

"""**Seach of best hyperparameters**"""

# rango=np.arange(0.1,1.0,0.1)
# rango

# param_grid = [
#     # try 12 (3×4) combinations of hyperparameters
#     {'n_estimators': [10, 20, 50, 100], 'max_features': [5, 8, 18], 'max_depth':[3,5,7,10]},
#     # then try 6 (2×3) combinations with bootstrap set as False
#     {'bootstrap': [False], 'n_estimators': [10, 30, 50], 'max_features': [5, 8, 18]},
#   ]

# RandomForestRegressor(random_state=42,n_estimators=5,max_depth=10),
#                   param_grid={'max_features': range(2, 50, 2)},
# forest_reg2 = RandomForestRegressor(random_state=42)
# # train across 5 folds, that's a total of (12+6)*5=90 rounds of training
# grid_search = GridSearchCV(forest_reg2, param_grid, cv=5,
#                            scoring='neg_mean_squared_error',
#                            return_train_score=True)
# grid_search.fit(train_prepared, y_train)

"""### Notes prediccions ###

#### 16/05/2023: I think something went wrong with the files ####

- RandomForestRegressor(n_estimators=20, random_state=42)
- MSE (train | test):
- 0.0013752198833196705 0.001560372795458149

#### 18/05/2023: 2022 data only ####
- RandomForestRegressor(n_estimators=20, random_state=42)
- MSE (train | test):
- 0.0020430191261473067 0.012574955517808432

#### 21/05/2023: ####
- RandomForestRegressor(n_estimators=20, random_state=42)
- MSE (train | test):
- 0.0022623862347308284 0.012974909618175656

- LinearRegression()
- MSE (train | test):
- 0.012224824865615715 0.012273542087721093

#### 22/05/2023: ####
- LinearRegression() + stratificat categoria 'disponibilitat'
- MSE (train | test):
- 0.012272287882953791 0.01208374240252064

#### 25/05/2023: global.df
- LinearRegression()
- MSE (train | test):
- 0.014008306896792399 0.014022284539283952

#### 26/05/2023: Nou fitxer training ####
- LinearRegression()
- MSE (train | test):
- 0.012078006010124561 0.012059598577670987

#### 30/05/2023:
#### stratificacio: mes,dia i hora
- LinearRegression()
- MSE (train | test):
- 0.01208940624657823 0.012013998523032027

#### stratificacio: categoria disponibilitat
- LinearRegression()
- MSE (train | test):
- 0.012114677756332919 0.011912930260635139

#### 03/06/2023:  stratificació disp + weather feat+weekend+peekhour (sense station_id ni capacity)
**LinearRegression**
- MSE (train | test):
- 0.011958292744636502 0.011760218193388865  --> submission set score: 0.12427

**Random Forest**
- MSE (train | test):
- 0.0015202273211973466 0.010681439728055818  --> submission set score: 0.36 :(
- R2 (train | test):
- 0.9796371574154732 0.8458634173568492

#### 08/06/2023:  stratificació disp + weather feat(nova versió) +weekend+peekhour (sense station_id ni capacity)
**LinearRegression**
- MSE (train | test):
- 0.01195826455205245 0.011758809500616585

**Random Forest**
- MSE (train | test):
- 0.0015281045979033243 0.010738671476996902  --> submission set score: 0.12107
- R2 (train | test):
- 0.9795423167314217 0.8453139091697832

#### 12/06/2023:  stratificació disp +['altitude','RS','PPT','VV10','n_transp_500m','capacity','month','hour','HR','T','min_dist_to_beach','post_code','ctx-4','ctx-3','ctx-2','ctx-1','weekend','peekhour','holiday']

**LinearRegression**
- MSE (train | test):
- 0.01195490799838289 0.011755580501641043

**Random Forest**
- MSE (train | test):
- 0.0014705172388749916 0.010343680402105112  --> submission set score: 0.11937
- R2 (train | test):
- 0.9803543453860426 0.8513882162744817

### Salvar el model ###
"""

# # Reload pipe
# lin_reg_fit = load('models/lin_reg_test.joblib')
# try:
#     lin_reg_fit
#     print(f'{datetime.now()}: Model is loaded.')
# except NameError:
#     print(f'{datetime.now()}: Model not defined.')

# # Make predictions
# y_train_pred_lrl = lin_reg_fit.predict(train_prepared)
# y_test_pred_lrl = lin_reg_fit.predict(test_prepared)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_pred_lrl, y_train), mean_squared_error(y_test_pred_lrl, y_test))

"""Ara ho provem salvant un pipeline amb tots els passos"""

# clf = Pipeline(
#     steps=[("add_features", add_features), ("col_transform",full_pipeline),("regressor", LinearRegression())]
# )
# clf.fit(train_prepared,y_train)
# dump(clf, 'models/pipelinetest.joblib')

# train_prepared

# train_prepared.shape

# def predict_bicing2023(subset, model):

#     X_pred = add_features.fit_transform(subset)
#     X_pred = full_pipeline.transform(X_pred)

#     y_pred = model.predict(X_pred)
#     #preparem el fitxer per pujar a kaggle
#     df_output = pd.DataFrame(y_pred)
#     df_output = df_output.reset_index()
#     df_output.columns = ['index','percentage_docks_available']
#     avui = datetime.today()
#     name = 'submission/submission-{}-{}-{}-{}.csv'.format(avui.year,avui.month,avui.day,avui.hour)
#     df_output.to_csv(name,index=False)

"""### Entrenem el fitxer global_df ###"""

glob = pd.read_csv(f'data_bicing/global_df.csv')
glob

# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error

# seed=42

# features=['ctx-4','ctx-3','ctx-2','ctx-1']

# X = glob[features].fillna(0)

# y = glob['percentage_docks_available'].fillna(0).copy()

# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=seed,shuffle=True)

# lin_reg = LinearRegression()
# lin_reg.fit(X_train[features], y_train)

# # Make predictions
# y_train_pred = lin_reg.predict(X_train)
# y_test_pred = lin_reg.predict(X_test)

# # Compute MSE for training and testing sets
# print('MSE (train | test):')
# print(mean_squared_error(y_train_pred, y_train), mean_squared_error(y_test_pred, y_test))

"""-LinearRegression() amb **global.df** i features=**['ctx-4','ctx-3','ctx-2','ctx-1']**

-MSE (train | test):
-0.014008306896792399 0.014022284539283952
"""

